- create a file Dockerfile
- insert the below codes into Dockerfile
    # In a Docker file each step is called a layer
    # Use the official Python image from the Docker Hub
    FROM python:3.12.5

    # Set the working directory in the container
    WORKDIR /usr/src/app

    # Copy the requirements.txt file into the container [this is a file thats why './']
    # This is used to copy a specific file (req.txt) to the container.
    # It's often done early in the Dockerfile to leverage Docker's caching mechanism.
    # If req.txt hasn't changed, Docker can use the cache for subsequent steps, speeding up the build process.

    COPY req.txt ./

    # Install the dependencies specified in the requirements file
    RUN pip install --no-cache-dir -r req.txt

    # Copy the rest of the application code into the container [this is a directory thats why '.']
    COPY . .

    # Specify the command to run the application
    CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]

- create a file .dockerignore
- insert the below codes into dockerignore file, add more if needed, based on the project.
    __pycache__
    env/
    .env
    Dockerfile
    .dockerignore
    .pytest_cache

- run: `docker build -t fastapi .` to build the docker image
    - -t <tags: string>, example: fastapi
    - . <Use the necessary docker files from the current directory>

- run `docker image ls` to see the list of latest build images

--------------------------------------------------------------------------------------
| Define and run multi-container applications with Docker                            |
| docker compose will automatically spin up/down our container with one cmd          |
--------------------------------------------------------------------------------------
| docker-compose.yml file will set up the necessary services and handle dependencies.|
--------------------------------------------------------------------------------------
| this is an alternative of docker run command, all the flags taken by run command   |
| can be set in the compose file, everytime we run compose file we do not need to set|
| the flags, all will be taken from the compose file. Follow the below steps.        |
--------------------------------------------------------------------------------------

- create docker-compose.yml file
- insert the below codes into docker-compose.yml
    # Define and run multi-container applications with Docker
    # version of docker compose file
    # version: '3.8'

    # each service will spin a container. need to spin more containers , more service need to be added
    services:
    #container name. ex: fastapi_service
    fastapi_service:
        #path to docker file, which will be used to build the image
        #we can point to a already build image as well here.
        build: .
        container_name: fastapi_container
        #outsider cannot talk to the container directly, so we are opening our localhost port and forwarding to the containet port.
        #bascially we saying to docker if we receive any traffic <port on localhost> will be forwarded to <port on container>
        #ex: <port on localhost>:<port on container>
        #http://localhost:<port on localhost>: port specified on the `Dockerfile` to run the application, in our case both are `8000`
        ports:
        - 8000:8000

        # The volumes directive in Docker Compose is used to mount directories from the host machine into Docker containers,
        # allowing file access and synchronization between the host and container.
        # ./:/usr/src/app: This mounts the ./ directory (where the compose file lives) from the host to /usr/src/app in the container.
        # Efficiency: Immediate reflection of code changes without rebuilding.
        # Data Management: Persistent data storage across container lifecycles.
        # Consistency: Ensures the container environment matches the host setup.
        # basically this hold your code and will keep looking for changes and will instantly sync changes w/o image rebuild
        # this volume will be kept for container restart
        # this is a `bind mount` volume
        volumes:
        - ./:/usr/src/app
        
        # Setting the env variable from file or list down, we are going with list down here.
        # env_file:
        #   - ./.env
        environment:
        #Docker creates its own network to communicate between its containers(services).
        #In our case we need to set the `IP to DB_HOSTNAME`, we can look for the `postgres_service` IP and set it to here.
        #Alternative way is just put the `service_name` docker will figure the IP itself.
        # - DATABASE_HOSTNAME=localhost
        - DATABASE_HOSTNAME=postgres_service
        - DATABASE_PORT=5432
        #db_password, db_name, db_usrname should be same as db enviroonment
        - DATABASE_PASSWORD=12345
        - DATABASE_NAME=fastapi
        - DATABASE_USERNAME=postgres
        - SECRET_KEY=09d25e094faa6ca2556c818166b7a9563b93f7099f6f0f4caa6cf63b88e8d3e7
        - ALGORITHM=HS256
        - ACCESS_TOKEN_EXPIRE_MINUTES=60

        # as we are using volume for this service and so, to refelect the changes we made in the development environment
        # to be effective in the container w/o rebuild the image we are using the below cmd with `--reload` flag
        command: uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
        #This service will start after <postgres_service>
        depends_on:
        - postgres_service

    # if you want a pgAdmin to inspect in your DB instance from the browser you can build the below container.
    # visit localhost:5050 to connect pgadmin from browser, use credentials from environment to login.
    # put <postgres_service> as host.
    # or if you want to inspect from Host Machine pgAdmin Desktop app you do not need this service
    # but you may need to stop postgres instance running in the Host machine or else you may got connected to the host machine instance
    # you will be able to use localhost as host in the pgadmin.
    # pgadmin_service:
    #   image: dpage/pgadmin4:latest
    #   container_name: pgadmin_container
    #   environment:
    #     PGADMIN_DEFAULT_EMAIL: admin@example.com
    #     PGADMIN_DEFAULT_PASSWORD: admin
    #   ports:
    #     - "5050:80"
    #   depends_on:
    #     - postgres_service


    #another service to run postgres instance a container
    postgres_service:
        # offcial image name
        image: postgres
        container_name: postgres_container
        #mandatory environment variable
        environment:
        #setting postgres_password is mandatory, rest are optional
        - POSTGRES_PASSWORD=12345
        - POSTGRES_DB=fastapi
        - POSTGRES_USER = postgres

        ports:
        - 5432:5432

        #when a containers goes down, db datas are also gone. To store and fetch db data after a shutdown and again restart
        #we can use a `named volume`, as depicted below
        #db-name:<path to data store>
        volumes:
        - postgres-db:/var/lib/postgresql/data

    #this global volume refering to the `named volume` above, so that any other container can access the inside this volume
    volumes:
    postgres-db:

- run: `docker-compose up -d` to run the container.
    - -d, --detach Detached mode: Run containers in the background
    - the naming systax will be <working-directory-name_service-name_serial-no>
    - dont use `-d` tag if you want to run the container in the cmd.

- run: `docker-compose up --build -d`: if you have any changes in the project
    and docker image already existed. Cause `docker-compose` cannot detec change.
    `--build` flag will build images before starting containers
    - to reflect the development changes instantly, an approach is given below.

- run: `docker logs <container_id> or <container_name>` to view its log
- run: `docker ps -a` to List all containers

- run: `docker-compose down` to shutdown the service/container

- run: `docker-compose down -v` to shutdown the service/container along volumes.
    this is important when you have a DB volume, and need to delete the volume and need to recreate.
    for example a DB volume is created and dont know what was the actual env variables when the db has created.
    cause you have changed env variables frequently for experiment purpose.
    so drop the volume and recreate again.

- after docker setup you need to setup DB tables.
    - open the terminal of fastapi service container, you can use docker desktop to open a terminal of specific service
    - cd to working directory.
    - run: `alembic upgrade head`, job is done.

- from now dont shutdown containers with `-v` flag, else you have to recreate the DB tables.


-----------to reflect the development changes instantly into container----------
- add the below (`volumes` and `command`) directives in the fastapi_service
    # The volumes directive in Docker Compose is used to mount directories from the host machine into Docker containers,
    # allowing file access and synchronization between the host and container.
    # ./:/usr/src/app: This mounts the ./ directory (where the compose file lives) from the host to /usr/src/app in the container.
    # Efficiency: Immediate reflection of code changes without rebuilding.
    # Data Management: Persistent data storage across container lifecycles.
    # Consistency: Ensures the container environment matches the host setup.
    # basically this hold your code and will keep looking for changes and will instantly sync changes w/o image rebuild
    # this volume will be kept for container restart
    volumes:
      - ./:/usr/src/app
    
    # as we are using volume for this service and so, to refelect the changes we made in the development environment
    # to be effective in the container w/o rebuild the image we are using the below cmd with `--reload` flag
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload


--------------Inspect into Postgres Instance from pgAdmin-------------------------
- inspect DB instance from the browser, build below container in the `docker-compose.yml` file
    pgadmin_service:
      image: dpage/pgadmin4:latest
      container_name: pgadmin_container
      environment:
        PGADMIN_DEFAULT_EMAIL: admin@example.com
        PGADMIN_DEFAULT_PASSWORD: admin
      ports:
        - "5050:80"
      depends_on:
        - postgres_service

    - visit localhost:5050 to connect pgadmin from browser, use credentials from environment to login.
    - put <postgres_service> as host.

- inspect DB instance from Host Machine pgAdmin Desktop app you do not need to build/add above service/container
    but you may need to stop postgres instance running in the Host machine(if any) or else you may got connected to the host machine instance,
    you will be able to use localhost as host in the pgadmin.

------------------------------------
|docker-compose file for DEV & PROD|
------------------------------------
In this stage of the project we are done with docker with last one step left that is:
clone `docker-compose.yml` file into two different file:
    - `docker-compose-dev.yml`  -> for development purpose
    - `docker-compose-prod.yml` -> for production purpose

- run: `docker-compose -f docker-compose-xxx.yml <regular-docker-compose-command>` to run docker compose
    command if your docker compose file does not contains the regular naming convention
    - `-f`: flag stands for `file`
    - ex: `docker-compose -f docker-compose-dev.yml up -d`

- `docker-compose-dev.yml` is our regular `docker-compose.yml` file we have developed till now
- `docker-compose-prod.yml` is our regular `docker-compose.yml` with few changes can be
    seen if you open the file.
    - we are no longer building images for our container, rather we have pushed our builded image in the docker
        dockerhub repo and pulling that image in the production.
        - dockerhub repo works as github repo for docker images.
    - removed hardcoded env variables and implemented mechanism to fetch env variables from the host machine.
    - removed `bind mount` volume, as do not need sync host directory with production container
    - removed removed `--reload` for fastapi_service not to look for changes.

---------------------------------
|Docker image push to docker-hub|
---------------------------------
- run: `docker-login` to login into docker from cmd. If you are logged in into the docker desktop you should be logged in instantly
- docker image name need to be same as docker-hub repo name. ex: <dockerhub-usrname/repo-name>
    - this can be set while building the docker image, which is not a commonly used practice.
    - so we will Tag our Docker image (if not done during build): If our image is already built but not tagged,
        you can tag it manually:
        - run: `docker tag existing-image-id yourusername/yourimagename:tag`
            - providing <:tag> value is optional, default will be optional
        - run: `docker push yourusername/yourimagename:tag`. job will be done.
