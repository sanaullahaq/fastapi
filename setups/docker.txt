- create a file Dockerfile
- insert the below codes into Dockerfile
    <contents can be foound inside Dockerfile file>

- create a file .dockerignore
- insert the below codes into dockerignore file, add more if needed, based on the project.
    <contents can be foound inside .dockerignore file>

- run: `docker build -t fastapi .` to build the docker image
    - -t <tags: string>, example: fastapi
    - . <Use the necessary docker files from the current directory>

- run `docker image ls` to see the list of latest build images

--------------------------------------------------------------------------------------
| Define and run multi-container applications with Docker                            |
| docker compose will automatically spin up/down our container with one cmd          |
--------------------------------------------------------------------------------------
| docker-compose.yml file will set up the necessary services and handle dependencies.|
--------------------------------------------------------------------------------------
| this is an alternative of docker run command, all the flags taken by run command   |
| can be set in the compose file, everytime we run compose file we do not need to set|
| the flags, all will be taken from the compose file. Follow the below steps.        |
--------------------------------------------------------------------------------------

- create docker-compose.yml file
- insert the below codes into docker-compose.yml
    <contents can be foound inside docker-compose.yml file>

- run: `docker-compose up -d` to run the container.
    - -d, --detach Detached mode: Run containers in the background
    - the naming systax will be <working-directory-name_service-name_serial-no>
    - dont use `-d` tag if you want to run the container in the cmd.

- run: `docker-compose up --build -d`: if you have any changes in the project
    and docker image already existed. Cause `docker-compose` cannot detec change.
    `--build` flag will build images before starting containers
    - to reflect the development changes instantly, (an approach is given below.)

- run: `docker logs <container_id> or <container_name>` to view its log
- run: `docker ps -a` to List all containers

- run: `docker-compose down` to shutdown the service/container

- run: `docker-compose down -v` to shutdown the service/container along volumes.
    this is important when you have a DB volume, and need to delete the volume and need to recreate.
    for example a DB volume is created and dont know what was the actual env variables when the db has created.
    cause you have changed env variables frequently for experiment purpose.
    so drop the volume and recreate again.

- after docker setup you need to setup DB tables.
    - open the terminal of fastapi service container, you can use docker desktop to open a terminal of specific service
    - cd to working directory.
    - run: `alembic upgrade head`, job is done.

- from now dont shutdown containers with `-v` flag, else you have to recreate the DB tables.


-----------to reflect the development changes instantly into container----------
- add the below (`volumes` and `command`) directives in the fastapi_service
    # The volumes directive in Docker Compose is used to mount directories from the host machine into Docker containers,
    # allowing file access and synchronization between the host and container.
    # ./:/usr/src/app: This mounts the ./ directory (where the compose file lives) from the host to /usr/src/app in the container.
    # Efficiency: Immediate reflection of code changes without rebuilding.
    # Data Management: Persistent data storage across container lifecycles.
    # Consistency: Ensures the container environment matches the host setup.
    # basically this hold your code and will keep looking for changes and will instantly sync changes w/o image rebuild
    # this volume will be kept for container restart
    volumes:
      - ./:/usr/src/app
    
    # as we are using volume for this service and so, to refelect the changes we made in the development environment
    # to be effective in the container w/o rebuild the image we are using the below cmd with `--reload` flag
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload


--------------Inspect into Postgres Instance from pgAdmin-------------------------
- inspect DB instance from the browser, build below container in the `docker-compose.yml` file
    pgadmin_service:
      image: dpage/pgadmin4:latest
      container_name: pgadmin_container
      environment:
        PGADMIN_DEFAULT_EMAIL: admin@example.com
        PGADMIN_DEFAULT_PASSWORD: admin
      ports:
        - "5050:80"
      depends_on:
        - postgres_service

    - visit localhost:5050 to connect pgadmin from browser, use credentials from environment to login.
    - put <postgres_service> as host.

- inspect DB instance from Host Machine pgAdmin Desktop app you DO NOT need to build/add above service/container
    but you may need to stop postgres instance running in the Host machine(if any) or else you may got connected to the host machine instance,
    you will be able to use localhost as host in the pgadmin.

------------------------------------
|docker-compose file for DEV & PROD|
------------------------------------
In this stage of the project we are done with docker with last one step left that is:
clone `docker-compose.yml` file into two different file:
    - `docker-compose-dev.yml`  -> for development purpose
    - `docker-compose-prod.yml` -> for production purpose

- run: `docker-compose -f docker-compose-xxx.yml <regular-docker-compose-command>` to run docker compose
    command if your docker compose file does not contains the regular naming convention
    - `-f`: flag stands for `file`
    - ex: `docker-compose -f docker-compose-dev.yml up -d`

- `docker-compose-dev.yml` is our regular `docker-compose.yml` file we have developed till now
- `docker-compose-prod.yml` is our regular `docker-compose.yml` with few changes can be
    seen if you open the file.
    - we are no longer building images for our container, rather we have pushed our builded image in the docker
        dockerhub repo and pulling that image in the production.
        - dockerhub repo works as github repo for docker images.
    - removed hardcoded env variables and implemented mechanism to fetch env variables from the host machine.
    - removed `bind mount` volume, as do not need sync host directory with production container
    - removed removed `--reload` for fastapi_service not to look for changes.

---------------------------------
|Docker image push to docker-hub|
---------------------------------
- run: `docker-login` to login into docker from cmd. If you are logged in into the docker desktop you should be logged in instantly
- docker image name need to be same as docker-hub repo name. ex: <dockerhub-usrname/repo-name>
    - this can be set while building the docker image, which is not a commonly used practice.
    - so we will Tag our Docker image (if not done during build): If our image is already built but not tagged,
        you can tag it manually:
        - run: `docker tag existing-image-id yourusername/yourimagename:tag`
            - providing <:tag> value is optional, default will be `latest`
        - run: `docker push yourusername/yourimagename:tag` and your job will be done.

    - thus we will be able to use the docker image
        - just inseatd of:
        - `build .` use `image: sanaullahaq/fastapi-prod`


------------------------------------------------------
|Last but no the least-----points not mentioned above|
------------------------------------------------------
- docker-compose/docker-compose-dev/docker-compose-prod contains the multi command `command` directive
    ```
    command: >
      sh -c "./wait-for-it.sh postgres_service:5432 -- alembic upgrade head && uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload"
    ```
    instead of
    ```
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
    ```

    with multi command `command` directive we will be able to wait until the postgres_service container is ready to take connection.
    It will save us from connection refused error. Also we do not need to create DB tables manually everytime we remove the volume.
    More detailed comments added in the .yml files.